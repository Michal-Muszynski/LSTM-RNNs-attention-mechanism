{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "# importing libraries \n",
    "import numpy as np\n",
    "from attention_utils import get_data # scirpt to genetate data\n",
    "import pandas as pd\n",
    "#from __future__ import print_function\n",
    "# import keras\n",
    "import keras\n",
    "#keras.__version__\n",
    "print(keras.__version__)\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "# all modules of keras for lSTM-RNNs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "#from keras.layers import Merge # problem with Merge because I now use Keras 2.2.4. Merge is replaced by add\n",
    "from keras.layers import add\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop,Adamax\n",
    "from keras import backend as K\n",
    "\n",
    "# turn off the warnings, be careful when use this\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading files\n",
    "# features\n",
    "X_test=pd.read_csv('Test_set_1L_AC_aud_feat_win5_lag1_gsr.csv', sep=',',header=None) # reading ok #X_test.values\n",
    "X_train=pd.read_csv('Train_set_1L_AC_aud_feat_win5_lag1_gsr.csv', sep=',',header=None)\n",
    "X_valid=pd.read_csv('Valid_set_1L_AC_aud_feat_win5_lag1_gsr.csv', sep=',',header=None)\n",
    "# labels\n",
    "Y_test=pd.read_csv('Test_set_synch_feat_win5_lag1_Awe.csv', sep=',',header=None)\n",
    "Y_train=pd.read_csv('Train_set_synch_feat_win5_lag1_Awe.csv', sep=',',header=None)\n",
    "Y_valid=pd.read_csv('Valid_set_synch_feat_win5_lag1_Awe.csv', sep=',',header=None)\n",
    "# small sets\n",
    "X_test_s=X_test[0:101]\n",
    "X_train_s=X_train[0:201]\n",
    "X_valid_s=X_valid[0:101]\n",
    "Y_test_s=Y_test[0:101]\n",
    "Y_train_s=Y_train[0:201]\n",
    "Y_valid_s=Y_valid[0:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n",
      "19428\n"
     ]
    }
   ],
   "source": [
    "number_of_features = X_train.shape[1] # columns are features , rows are observations\n",
    "print(number_of_features)\n",
    "number_of_observations = X_train.shape[0] # columns are features , rows are observations\n",
    "print(number_of_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape the panda.DataFrame format data to Keras style: (batch_size, time_step, nb_features)\n",
    "def reshape_data(data, n_prev): #n_prev = time_step \n",
    "    docX = []\n",
    "    for i in range(len(data)):\n",
    "        if i < (len(data)-n_prev):\n",
    "            docX.append(data.iloc[i:i+n_prev].as_matrix())\n",
    "        else: # the frames in the last window use the same context\n",
    "            docX.append(data.iloc[(len(data)-n_prev):len(data)].as_matrix())\n",
    "    alsX = np.array(docX)\n",
    "    return alsX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom evaluation metrics\n",
    "def pearson_cc(y_true, y_pred):\n",
    "    fsp = y_pred - K.mean(y_pred,axis=0)   \n",
    "    fst = y_true - K.mean(y_true,axis=0) \n",
    "    devP = K.std(y_pred,axis=0)  \n",
    "    devT = K.std(y_true,axis=0)\n",
    "\n",
    "    return K.sum(K.mean(fsp*fst,axis=0)/(devP*devT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model: model\n",
    "def build_LSTM_model(time_step, nb_features, H1, dropout_W1, dropout_U1,H2, dropout_W2, dropout_U2, H3, dropout_W3, dropout_U3):\n",
    "    # define variables\n",
    "# nb_features = 1000 # total number of features, also the number of neurons in the input layer of LSTM\n",
    "# time_step = 3  # the length of history (number of previous data instances) to include\n",
    "# H1 = 128 # number of neurons in the bottom hidden layer\n",
    "# H2 = 64 # number of neurons in the middle hidden layer\n",
    "# H3 = 32 # number of neurons in the top hidden layer\n",
    "# dropout_W1 = 0.5 # drop out weight (for preventing over-fitting) in H1\n",
    "# dropout_U1 = 0.5 # drop out weight (for preventing over-fitting) in H1\n",
    "# dropout_W2 = 0 # drop out weight (for preventing over-fitting) in H2\n",
    "# dropout_U2 = 0 # drop out weight (for preventing over-fitting) in H2\n",
    "# dropout_W3 = 0 # drop out weight (for preventing over-fitting) in H3\n",
    "# dropout_U3 = 0 # drop out weight (for preventing over-fitting) in H3\n",
    "    # define model structure\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(H1, input_shape=(time_step, nb_features),dropout_W=dropout_W1, dropout_U=dropout_U1, return_sequences=True)) \n",
    "    # bottom hidden layer\n",
    "    model.add(LSTM(H2, dropout_W=dropout_W2, dropout_U=dropout_U2, return_sequences=True)) # middle hidden layer\n",
    "    model.add(LSTM(H3, dropout_W=dropout_W3, dropout_U=dropout_U3, return_sequences=False)) # top hidden layer\n",
    "    model.add(Dense(1, activation='sigmoid')) # output layer, regression task, value range [-1,1]\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization algorithms\n",
    "def opt_LSTM_model(lr):\n",
    "    #opt_func = RMSprop(lr=0.0001) # training function\n",
    "    # lr=0.0005 # learning rate for the optimization algorithm \n",
    "    opt_func = Adamax(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    return opt_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "nb_features = number_of_features # total number of features, also the number of neurons in the input layer of LSTM\n",
    "time_step = 3  # the length of history (number of previous data instances) to include\n",
    "batch_size = 32 # the model performs a weight update for every batch, smaller batch means faster learning but les stable weights\n",
    "nb_epoch = 1000 # number of total epochs to train the model\n",
    "H1 = 128 # number of neurons in the bottom hidden layer\n",
    "H2 = 64 # number of neurons in the middle hidden layer\n",
    "H3 = 32 # number of neurons in the top hidden layer\n",
    "dropout_W1 = 0.5 # drop out weight (for preventing over-fitting) in H1\n",
    "dropout_U1 = 0.5 # drop out weight (for preventing over-fitting) in H1\n",
    "dropout_W2 = 0 # drop out weight (for preventing over-fitting) in H2\n",
    "dropout_U2 = 0 # drop out weight (for preventing over-fitting) in H2\n",
    "dropout_W3 = 0 # drop out weight (for preventing over-fitting) in H3\n",
    "dropout_U3 = 0 # drop out weight (for preventing over-fitting) in H3\n",
    "lr=0.0005 # learning rate for the optimization algorithm \n",
    "\n",
    "\n",
    "# function that merge testing, validation, optimzation settings, and compilation\n",
    "def train_valid_model(nb_features,time_step,batch_size,nb_epoch,H1, dropout_W1, dropout_U1,H2, dropout_W2, dropout_U2, H3, dropout_W3, dropout_U3,lr):\n",
    "\n",
    "    LSTM_model = build_LSTM_model(time_step, nb_features, H1, dropout_W1, dropout_U1,H2, dropout_W2, dropout_U2, H3, dropout_W3, dropout_U3)\n",
    "    opt_func = opt_LSTM_model(lr)\n",
    "    # Compiling\n",
    "    LSTM_model.compile(loss=pearson_cc, optimizer=opt_func, metrics=[pearson_cc,'mse']) \n",
    "    # define the optimizer for training, use cc and mse as the evaluation metric\n",
    "    # carry out training\n",
    "\n",
    "    # Reshaping the panda.DataFrame format data to Keras style: (nb_observations, time_step, nb_features)\n",
    "    # shape (nb_observations, nb_features)\n",
    "    #X_Test = reshape_data(X_test)\n",
    "    #X_Train = reshape_data(X_train)\n",
    "    #X_Valid = reshape_data(X_valid)\n",
    "    # small set\n",
    "    X_Test = reshape_data(X_test_s,time_step)\n",
    "    X_Train = reshape_data(X_train_s,time_step)\n",
    "    X_Valid = reshape_data(X_valid_s,time_step)\n",
    "    # after reshaping: shape (nb_observations,time_step,nb_features)\n",
    "\n",
    "    # if the validation loss isn't decreasing for a number of epochs, stop training to prevent over-fitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    LSTM_model.fit(X_Train, Y_train_s, batch_size=batch_size, nb_epoch=nb_epoch, validation_data=(X_Valid, Y_valid_s), callbacks=[early_stopping], verbose=0)\n",
    "    return LSTM_model\n",
    "\n",
    "# training the model using validation set\n",
    "LSTM_model=train_valid_model(nb_features,time_step,batch_size,nb_epoch,H1, dropout_W1, dropout_U1,H2, dropout_W2, dropout_U2, H3, dropout_W3, dropout_U3,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(LSTM_model, X_Test, Y_test, batch_size):\n",
    "    # evaluation\n",
    "    test_score, test_cc, test_mse = LSTM_model.evaluate(X_Test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print('Test cc:', test_cc)\n",
    "    print('Test mse:', test_mse)\n",
    "    print('\\n')\n",
    "    # output predictions\n",
    "    test_prediction = LSTM_model.predict(X_Test)\n",
    "    test_prediction_df = pd.DataFrame(test_prediction)\n",
    "    return test_prediction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cc: 0.04961800634270847\n",
      "Test mse: 0.00949685377216354\n",
      "\n",
      "\n",
      "            0\n",
      "0    0.467780\n",
      "1    0.472603\n",
      "2    0.477776\n",
      "3    0.481069\n",
      "4    0.478078\n",
      "5    0.476572\n",
      "6    0.477086\n",
      "7    0.483431\n",
      "8    0.487802\n",
      "9    0.483441\n",
      "10   0.479389\n",
      "11   0.480100\n",
      "12   0.483656\n",
      "13   0.487532\n",
      "14   0.489766\n",
      "15   0.489046\n",
      "16   0.488323\n",
      "17   0.491141\n",
      "18   0.494248\n",
      "19   0.488238\n",
      "20   0.483318\n",
      "21   0.480842\n",
      "22   0.488443\n",
      "23   0.499180\n",
      "24   0.506261\n",
      "25   0.503984\n",
      "26   0.498233\n",
      "27   0.492405\n",
      "28   0.485391\n",
      "29   0.479225\n",
      "..        ...\n",
      "71   0.501757\n",
      "72   0.503246\n",
      "73   0.504472\n",
      "74   0.504564\n",
      "75   0.509212\n",
      "76   0.513236\n",
      "77   0.512897\n",
      "78   0.509642\n",
      "79   0.500684\n",
      "80   0.489340\n",
      "81   0.492384\n",
      "82   0.507265\n",
      "83   0.517534\n",
      "84   0.517531\n",
      "85   0.507962\n",
      "86   0.497797\n",
      "87   0.492670\n",
      "88   0.490747\n",
      "89   0.489687\n",
      "90   0.487853\n",
      "91   0.485782\n",
      "92   0.482967\n",
      "93   0.482443\n",
      "94   0.481944\n",
      "95   0.480962\n",
      "96   0.482148\n",
      "97   0.489965\n",
      "98   0.500279\n",
      "99   0.500279\n",
      "100  0.500279\n",
      "\n",
      "[101 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# output predictions\n",
    "test_pred_df=test_model(LSTM_model,X_Test, Y_test_s,batch_size)\n",
    "test_pred_df.to_csv('prediction.csv', mode='a', index=False, header=False)\n",
    "print(test_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
